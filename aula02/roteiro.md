# üé• Aula 02 - Criando um Chat M√©dico com LangChain e Streamlit

---
## üü¢ Introdu√ß√£o (Contextualiza√ß√£o)

Ol√°, pessoal! Bem-vindos √† segunda aula do nosso curso completo de LangChain. Hoje, vamos sair do terminal e criar nossa primeira aplica√ß√£o web, usando **Streamlit**. Essa aplica√ß√£o ser√° um **Chat M√©dico**, onde o usu√°rio informa sintomas e recebe uma orienta√ß√£o inicial de um assistente m√©dico simulado pelo modelo de linguagem.

---
## üü¢ Objetivo

O objetivo √© criar uma interface simples de chat, onde:
- O usu√°rio escreve seus sintomas.
- O modelo de IA (executado via **LangChain e ChatOllama**) analisa os sintomas e d√° um retorno.
- As mensagens s√£o exibidas em formato de chat, com hist√≥rico, igual em aplicativos reais de conversa.

---
## üü¢ Instalando as Depend√™ncias

> Lembre de usar o ambiente virtual ativo, .venv

Instale o Streamlit com:
```bash
pip install streamlit
```

Instale o LangChain Ollama:
```bash
pip install langchain-ollama
```

Para o Hist√≥rico de mensagens, instale o LangChain Core:
```bash
pip install langchain-core
```

---
## üü¢ Passo a passo do c√≥digo

### 1Ô∏è‚É£ Configura√ß√£o inicial da p√°gina Streamlit

```python
st.set_page_config(page_title="Chat WebDoctor")
st.header("Chat WebDoctor")
```

- Configura a p√°gina com t√≠tulo e cabe√ßalho personalizados.

---
### 2Ô∏è‚É£ Configurando o hist√≥rico de chat

```python
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = [
        AIMessage("Bem-Vindo ao WebDoctor! Quais as suas queixas?")
    ]
```

- Armazena o hist√≥rico de mensagens no `session_state`, come√ßando com uma mensagem inicial.

---
### 3Ô∏è‚É£ Exibindo o hist√≥rico de mensagens

```python
chat_history = st.session_state["chat_history"]

for history in chat_history:
    if isinstance(history, AIMessage):
        st.chat_message("ai").write(history.content)
    if isinstance(history, HumanMessage):
        st.chat_message("human").write(history.content)
```

- Mostra as mensagens anteriores no formato de chat (IA e usu√°rio).

---
### 4Ô∏è‚É£ Capturando a mensagem do usu√°rio

```python
sintomas = st.chat_input("Aguardando sua resposta...")
```

- Input para o usu√°rio digitar seus sintomas.

---
### 5Ô∏è‚É£ Adicionando a mensagem do usu√°rio ao hist√≥rico

```python
if sintomas:
    st.chat_message("user").write(sintomas)
    st.session_state["chat_history"] += [HumanMessage(sintomas)]
```

- Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe no chat.

---
### 6Ô∏è‚É£ Montando o prompt (PromptWebDoctor)

```python
prompt = PromptWebDoctor.prompt_inicial(sintomas)
```

- Usa uma fun√ß√£o externa para montar o prompt personalizado para o modelo.

Exemplo do arquivo Prompt_Web_Doctor.py:

```python
class PromptWebDoctor:
    @staticmethod
    def prompt_inicial(sintomas):
        return f"""
        Voc√™ √© um m√©dico assistente virtual especializado em fornecer orienta√ß√µes iniciais.
        O paciente informou os seguintes sintomas:
        {sintomas}
        Com base nesses sintomas, forne√ßa uma an√°lise inicial, poss√≠veis causas e oriente se √© necess√°rio buscar um m√©dico presencialmente.
        """
```

---
### 7Ô∏è‚É£ Inicializando o modelo ChatOllama

```python
llm = ChatOllama(model="llama3.2:1b", temperature=0.7)
```

- Configura o modelo a ser usado e sua temperatura.

---
### 8Ô∏è‚É£ Gerando resposta com stream

```python
output = llm.stream(prompt)
```

- A resposta √© gerada e transmitida em tempo real.

---
### 9Ô∏è‚É£ Exibindo a resposta da IA

```python
with st.chat_message("ai"):
    ai_message = st.write_stream(output)
```

- Exibe a resposta do modelo como uma mensagem no chat.

---
### üî†10Ô∏è‚É£ Salvando a resposta no hist√≥rico

```python
st.session_state["chat_history"] += [AIMessage(ai_message)]
```

- Adiciona a resposta da IA no hist√≥rico.

---
## üü¢ Conclus√£o e Demonstra√ß√£o

Para rodar a aplica√ß√£o:

```bash
streamlit run app.py
```

Demonstre:
- Enviar sintomas.
- Ver a resposta da IA.
- Continuar a conversa.

---
## üü¢ Resumo visual

| Etapa                   | C√≥digo                          |
|-------------------|--------------------|
| Configurar p√°gina | `st.set_page_config` e `st.header` |
| Hist√≥rico inicial   | `st.session_state` |
| Mostrar hist√≥rico   | `for history in chat_history` |
| Capturar input        | `st.chat_input()` |
| Adicionar hist√≥rico  | `chat_history += [...]` |
| Gerar resposta         | `llm.stream()` |
| Exibir resposta         | `st.write_stream()` |
| Salvar resposta       | `chat_history += [...]` |

---
## üü¢ Fechamento

Essa √© uma base simples, mas poderosa. No futuro, podemos expandir com:
- Melhor hist√≥rico.
- Personaliza√ß√£o de prompt.
- Integra√ß√£o com outros modelos.

---

